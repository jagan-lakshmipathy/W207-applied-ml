{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 13: Recommender Systems\n",
    "\n",
    "Instructor: Cornelia Ilin <br>\n",
    "Email: cilin@ischool.berkeley.edu <br>\n",
    "\n",
    "\n",
    "Citations: <br>\n",
    " - https://towardsdatascience.com/recommendation-system-in-python-lightfm-61c85010ce17 [Our example is stolen from here]\n",
    " - https://machinelearningmastery.com/sparse-matrices-for-machine-learning/\n",
    " - https://making.lyst.com/lightfm/docs/home.html#how-to-cite\n",
    " - https://realpython.com/build-recommendation-engine-collaborative-filtering/\n",
    " - https://developers.google.com/machine-learning/recommendation [Google's course on Recommender Systems, highly recommended!!]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    " - Review of main recommender systems concepts.\n",
    " - Goodreads example: focus on the LightFM package.\n",
    " - Final thoughts on W207 - Applied Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I usually present the notebook to the class and I ask questions along the way. Today, we will do a flipped-classroom exercise, as follows:\n",
    "\n",
    " - form teams of 3-4 people.\n",
    " - as a team, fill in the blanks for section [Review of main concepts].\n",
    " - go over the notebook; make sure you understand what's going on at each step; pay attention to all the summary stats.\n",
    " - explain the book_recommendation_system() function; again, make sure you understand what's implemented at each step.\n",
    " - finally, I will randomly choose a team to present the notebook to the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Review of main concepts\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "print('Source: https://towardsdatascience.com/recommendation-system-in-python-lightfm-61c85010ce17')\n",
    "Image(filename='recom_systems.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ``_________ filtering``, if user A is similar to user B, and user B likes video 1, then the system can recommend video 1 to user A (even if user A hasn’t seen any videos similar to video 1).\n",
    "\n",
    "``_________ filtering`` is well suited to complex domains where items are not purchased very often, such as apartments and cars. It is based not on a user’s rating history, but on specific queries made by the user.\n",
    "\n",
    "In ``_________ filtering``, if user A watches two cute cat videos, then the system can recommend cute animal videos to that user.\n",
    "\n",
    "``_________ filtering`` is also referred to as neighborhood-based collaborative filtering algorithms, where ratings of user-item combinations are predicted based on their neighborhoods. These neighborhoods can be further defined as (1) User Based, and (2) Item Based.\n",
    "\n",
    "In ``_________ filtering``, ML techniques are used to learn model parameters within the context of a given optimization framework.\n",
    "\n",
    "``_________ filtering`` is a special kind of recommender that uses both collaborative and content based filtering for making recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will be focusing on an example that uses ``Hybrid Filtering``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Goodreads example (LightFM package)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightfm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7h/fmlz356s7l70cmbksjxd2b7w0000gn/T/ipykernel_3790/3927220961.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# recommender systems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlightfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom_train_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlightfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_at_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_at_k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlightfm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLightFM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightfm'"
     ]
    }
   ],
   "source": [
    "# standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_profiling\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "# display, plots\n",
    "from IPython.display import display_html\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "# recommender systems\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "from lightfm.evaluation import auc_score, precision_at_k, recall_at_k\n",
    "from lightfm import LightFM\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_recommendation_user(model, user_book_interaction, user_id, user_dict, \n",
    "                               item_dict,threshold = 0,nrec_items = 5, show = True):\n",
    "    \"\"\" Define function here\n",
    "    # param\n",
    "    # return\n",
    "    \"\"\"\n",
    "    # model prediction for user_id\n",
    "    n_users, n_items = user_book_interaction.shape\n",
    "    user_x = user_dict[user_id]\n",
    "    scores = pd.Series(model.predict(user_x,np.arange(n_items), item_features=books_metadata_csr))\n",
    "    scores.index = user_book_interaction.columns\n",
    "    scores = list(pd.Series(scores.sort_values(ascending=False).index))\n",
    "    \n",
    "    # known items for user_id\n",
    "    known_items = list(pd.Series(user_book_interaction.loc[user_id,:] \\\n",
    "                                 [user_book_interaction.loc[user_id,:] > threshold].index).sort_values(ascending=False))\n",
    "    \n",
    "    # recommended items for user_id\n",
    "    scores = [x for x in scores if x not in known_items]\n",
    "    return_score_list = scores[0:nrec_items]\n",
    "    known_items = list(pd.Series(known_items).apply(lambda x: item_dict[x]))\n",
    "    scores = list(pd.Series(return_score_list).apply(lambda x: item_dict[x]))\n",
    "    \n",
    "    if show == True:\n",
    "        print (\"User: \" + str(user_id))\n",
    "        print(\"Known Likes:\")\n",
    "        counter = 1\n",
    "        for i in known_items:\n",
    "            print(str(counter) + '- ' + i)\n",
    "            counter+=1\n",
    "            \n",
    "        print(\"\\n Recommended Items:\")\n",
    "        counter = 1\n",
    "        for i in scores:\n",
    "            print(str(counter) + '- ' + i)\n",
    "            counter+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Read Goodreads data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets were collected in late 2017 from goodreads.com (see first citation). They only scraped users' public shelves, i.e. everyone can see it on web without login. User IDs and review IDs are anonymized. \n",
    "\n",
    "Two datasets: \n",
    " - meta-data of the books, \n",
    " - user-book interactions (users' public shelves). \n",
    "\n",
    "These datasets can be merged together by matching book/user/review ids. \n",
    "\n",
    "You can download the datasets from here (make sure to put in the same folder with this notebook or set the path accordingly):\n",
    " - meta-data: https://drive.google.com/uc?id=1H6xUV48D5sa2uSF_BusW-IBJ7PCQZTS1\n",
    " - user-book interactions: https://drive.google.com/uc?id=17G5_MeSWuhYnD4fGJMvKRSOlBqCCimxJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_metadata = pd.read_json('goodreads_books_poetry.json', lines=True)\n",
    "interactions = pd.read_json('goodreads_interactions_poetry.json', lines=True)\n",
    "\n",
    "print('Size of books metadata', books_metadata.shape)\n",
    "print('Size of user-book interactions data', interactions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4: Data visualization and preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ``Books Metadata``\n",
    "\n",
    "#### (1) Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_metadata.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Focus on a subsample of features\n",
    "but feel free to experiment with the others on your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['title','book_id', 'average_rating', 'is_ebook', 'num_pages', \n",
    "            'publication_year', 'ratings_count', 'language_code']\n",
    "\n",
    "books_metadata = books_metadata[features]\n",
    "books_metadata.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Visualize the data using pandas_profiling\n",
    "\n",
    "pandas_profiling generates profile reports from a pandas DataFrame. \n",
    "\n",
    "Citing from their own website, \"the pandas df.describe() function is great but a little basic for serious exploratory data analysis. pandas_profiling extends the pandas DataFrame with df.profile_report() for quick data analysis.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for meaningful pandas_profiling statistics, replace blank cells with NaN (check to see how the stats differ if you don't do this)\n",
    "books_metadata.replace('', np.nan, inplace=True)\n",
    "\n",
    "# select features for summary stats\n",
    "profile = pandas_profiling.ProfileReport(books_metadata[['average_rating', 'is_ebook', 'num_pages', \n",
    "                                                                  'publication_year', 'ratings_count']]) \n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Preprocess this data\n",
    "- Feature transformation:\n",
    "    - Replace the missing value of categorical values with another value to create a new category\n",
    "    - Convert bin values for numeric variables into discrete intervals\n",
    "- Perform one-hot encoding on the data\n",
    "- Convert dense matrix to sparse matrix\n",
    "- Create a book dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by transforming the value of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pandas cut method to convert fields into discrete intervals\n",
    "books_metadata['num_pages'].replace(np.nan, -1, inplace=True)\n",
    "books_metadata['num_pages'] = pd.to_numeric(books_metadata['num_pages'])\n",
    "books_metadata['num_pages'] = pd.cut(books_metadata['num_pages'], bins=25)\n",
    "\n",
    "# rounding ratings to nearest .5 score\n",
    "books_metadata['average_rating'] = books_metadata['average_rating'].apply(lambda x: round(x*2)/2)\n",
    "\n",
    "# using pandas qcut method to convert fields into quantile-based discrete intervals\n",
    "books_metadata['ratings_count'] = pd.qcut(books_metadata['ratings_count'], 25)\n",
    "\n",
    "# replacing missing values to year 2100\n",
    "books_metadata['publication_year'].replace(np.nan, 2100, inplace=True)\n",
    "\n",
    "# replacing missing values to 'unknown'\n",
    "books_metadata['language_code'].replace(np.nan, 'unknown', inplace=True)\n",
    "\n",
    "# convert is_ebook column into 1/0 where true=1 and false=0\n",
    "books_metadata['is_ebook'] = books_metadata.is_ebook.map(lambda x: 1.0*(x == 'true'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the data after feature transformation\n",
    "books_metadata.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's perform a one-hot encoding on this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_metadata = pd.get_dummies(books_metadata, columns = ['average_rating', 'is_ebook', 'num_pages', \n",
    "                                                           'publication_year', 'ratings_count', \n",
    "                                                           'language_code'])\n",
    "\n",
    "books_metadata = books_metadata.sort_values('book_id').reset_index().drop('index', axis=1)\n",
    "books_metadata.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_metadata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that now we have a lot more features in the books_metadata. At first sight it seems that there may be lots of zeros, so we need to investigate this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Question:`` What is the difference between a dense vs. sparse matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The sparsity of the books_metadata is:',\n",
    "      round((np.size(books_metadata)-np.count_nonzero(books_metadata))/np.size(books_metadata) *100,2),\n",
    "      'percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "``Sparse vs. Dense matrices``\n",
    "\n",
    "This is clearly a waste of memory resources as those zero values do not contain any information.\n",
    "\n",
    "\n",
    "Simply, if the matrix contains mostly zero-values, i.e. no data, then performing operations across this matrix may take a long time where the bulk of the computation performed will involve adding or multiplying zero values together.\n",
    "\n",
    "Sparse matrices turn up a lot in applied ML!\n",
    "\n",
    "Sparse matrices come up in encoding schemes used in the preparation of data. Three common examples include:\n",
    "\n",
    " - One-hot encoding, used to represent categorical data as sparse binary vectors [this is what we did above].\n",
    " - Count encoding, used to represent the frequency of words in a vocabulary for a document\n",
    " - TF-IDF encoding, used to represent normalized word frequency scores in a vocabulary.\n",
    " \n",
    "The solution to representing and working with sparse matrices is to use an alternate data structure to represent the sparse data.\n",
    "\n",
    "\n",
    "Two common data structures are:\n",
    "- Compressed Sparse Row (CSR). The sparse matrix is represented using three one-dimensional arrays for the non-zero values, the extents of the rows, and the column indexes.\n",
    "- Compressed Sparse Column (CSC). The same as the Compressed Sparse Row method except the column indices are compressed and read first before the row indices.\n",
    "\n",
    "CSR is very popular in ML.\n",
    "\n",
    "The idea is to ignore the zero values and focus only on the non-zero values.\n",
    "\n",
    "SciPy provides tools for creating sparse matrices using multiple data structures, as well as tools for converting a dense matrix to a sparse matrix. So this is what we will be using today.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's transform the books_metadata into a CSR sparse matrix that can be used for matrix operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_metadata_csr = csr_matrix(books_metadata.drop(['book_id','title'], axis=1).values)\n",
    "books_metadata_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(books_metadata_csr).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create a dictionary of book titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_dict ={}\n",
    "df = books_metadata[['book_id', 'title']].sort_values('book_id').reset_index()\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    item_dict[(df.loc[i,'book_id'])] = df.loc[i,'title']\n",
    "\n",
    "# print first 5 items:\n",
    "for item in list(item_dict)[0:5]:\n",
    "    print (item, item_dict[item])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ``User-book interactions data``\n",
    "\n",
    "#### (1) Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Focus on a subsample of features\n",
    "but feel free to experiment with the others on your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the books metadata to selected features\n",
    "features = ['user_id', 'book_id', 'is_read', 'rating']\n",
    "interactions= interactions[features]\n",
    "\n",
    "interactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Visualize data using pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for meaningful pandas_profiling statistics, replace blank cells with NaN\n",
    "interactions.replace('', np.nan, inplace=True)\n",
    "\n",
    "profile = pandas_profiling.ProfileReport(interactions[['is_read', 'rating']])\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Preprocess this data\n",
    "\n",
    "- Feature transformation:\n",
    "    - map boolean values (is_read) to string\n",
    "    - vonvert is_read column to 1/0\n",
    "    - check is_read and rating consistency\n",
    "    select a subsample of this data (RAM concerns)\n",
    "- Perform one-hot encoding on the data\n",
    "- Convert dense matrix to sparse matrix\n",
    "- Create a user dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by transforming the value of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping boolean to string\n",
    "booleanDictionary = {True: 'true', False: 'false'}\n",
    "interactions['is_read'] = interactions['is_read'].replace(booleanDictionary)\n",
    "\n",
    "# convert is_read column into 1/0 where true=1 and false=0\n",
    "interactions['is_read'] = interactions.is_read.map(lambda x: 1.0*(x == 'true'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have two fields denoting interaction between a user and a book, `is_read` and `rating` - let's see how many data points we have where the user hasn't read the book but have given the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions.groupby(['rating', 'is_read']).size().reset_index().pivot(columns='rating', index='is_read', values=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can conlude that there is consitency between is_read and rating because ratings >= 1 have all read the book. \n",
    "\n",
    "Finally, for the feature transformation step we will:\n",
    "- drop interactions where ``is_read`` is false\n",
    "- limit interactions from random 5000 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop if is_read == false\n",
    "interactions = interactions.loc[interactions['is_read']==1, ['user_id', 'book_id', 'rating']]\n",
    "\n",
    "# randomly select 5000 users\n",
    "interactions = interactions[interactions['user_id'].isin(\n",
    "               random.sample(list(interactions['user_id'].unique()), k=5000))]\n",
    "\n",
    "# print first 5\n",
    "interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Question:`` Hoe many users did we drop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's perform a user_book interaction (a form of hot-encoding) on this data. Note that if they read the book but did not provide any rating, then `rating`=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on-hot encoding (0/1 representation)\n",
    "user_book_interaction = pd.pivot_table(interactions, index='user_id', columns='book_id', values='rating')\n",
    "\n",
    "# fill missing values with 0\n",
    "user_book_interaction = user_book_interaction.fillna(0)\n",
    "\n",
    "user_book_interaction.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_book_interaction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the data seems to be filled with lots of zeros. Let's check the sparsity of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The sparsity of the books_metadata is:',\n",
    "      round((np.size(user_book_interaction)-np.count_nonzero(user_book_interaction))/np.size(user_book_interaction) *100,2),\n",
    "      'percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a CSR spare matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to csr matrix\n",
    "user_book_interaction_csr = csr_matrix(user_book_interaction.values)\n",
    "user_book_interaction_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize as data frame\n",
    "pd.DataFrame(user_book_interaction_csr).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a list of tuples is stored with each tuple containing the row index, column index, and the non-zero value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create a user dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = list(user_book_interaction.index)\n",
    "user_dict = {}\n",
    "counter = 0 \n",
    "for i in user_id:\n",
    "    user_dict[i] = counter\n",
    "    counter += 1\n",
    "\n",
    "# print first 5 items:\n",
    "for item in list(user_dict)[0:5]:\n",
    "    print (item, user_dict[item])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5: Model training\n",
    "---\n",
    "\n",
    "Ideally, we would build a train and test set, and evaluate several models for our recommender system to determine which model holds the most promise for further optimization (hyperparameter tuning).\n",
    "\n",
    "Today we will train a base model, with randomly selected input parameters (play with these on your own!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightFM(loss='warp',\n",
    "                random_state=0,\n",
    "                learning_rate=0.90,\n",
    "                no_components=150,\n",
    "                user_alpha=0.000005)\n",
    "\n",
    "model = model.fit(user_book_interaction_csr,\n",
    "                  epochs=100,\n",
    "                  num_threads=16, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 6: Find Top 5 book recommendations for a user\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define user_id\n",
    "user_id = 0\n",
    "for i, key in enumerate(user_dict.keys()):\n",
    "    if i==1234:\n",
    "        user_id=key\n",
    "\n",
    "# find book recommendations\n",
    "book_recommendation_user(model, user_book_interaction, user_id, user_dict, item_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w207_venv",
   "language": "python",
   "name": "w207_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
